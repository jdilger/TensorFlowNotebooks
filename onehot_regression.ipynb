{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "onehot_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2C9Q+ZG+enIVZcK2ZSNVR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdilger/TensorFlowNotebooks/blob/master/onehot_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SyY3WjXRjoYR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = [[0, 1, 2],\n",
        "           [4, 5, 1],\n",
        "           [0, 1, 2]]\n",
        "depth = 6\n",
        "tf.one_hot(indices, depth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMSa2k9GjsFG",
        "outputId": "e7d66027-3eb3-44a0-d91d-e66f182230ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3, 6), dtype=float32, numpy=\n",
              "array([[[1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 1.],\n",
              "        [0., 1., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[1., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0.]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import ee\n",
        "try:\n",
        "  ee.Initialize()\n",
        "except:\n",
        "  ee.Authenticate()\n",
        "  ee.Initialize()\n",
        "\n",
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "0oD-9HaNldc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify cloud storage bucket to save data too\n",
        "BUCKET = 'landfire'\n",
        "\n",
        "# Specify names locations for outputs in Cloud Storage. \n",
        "FOLDER = 'lucas/pools2fire_tf'\n",
        "TRAINING_BASE = 'training_patches_v11'\n",
        "TESTING_BASE = 'testing_patches_v11'\n",
        "VAL_BASE = 'val_patches_v4'\n",
        "\n",
        "BANDS = ['agfast', 'agmed', 'agslo', 'agvfast', 'bgfast', 'bgslo', 'bgvfast',\n",
        "         'croot', 'froot', 'foliage', 'merch', 'otherw', 'snbran', 'snstem',\n",
        "         'ldist', 'elevation', 'slope', 'aspect', 'age']\n",
        "CATEGORICAL = ['pyrome_freq']\n",
        "CATEGORICAL_DICt = {\n",
        "    'pyrome_freq':10\n",
        "}\n",
        "RESPONSE = ['BP']\n",
        "FEATURES = CATEGORICAL + BANDS + RESPONSE\n",
        "FEATURES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TemJ8za5lmlk",
        "outputId": "2c932f30-166a-4f65-8050-49c5d8fe9efc"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pyrome_freq',\n",
              " 'agfast',\n",
              " 'agmed',\n",
              " 'agslo',\n",
              " 'agvfast',\n",
              " 'bgfast',\n",
              " 'bgslo',\n",
              " 'bgvfast',\n",
              " 'croot',\n",
              " 'froot',\n",
              " 'foliage',\n",
              " 'merch',\n",
              " 'otherw',\n",
              " 'snbran',\n",
              " 'snstem',\n",
              " 'ldist',\n",
              " 'elevation',\n",
              " 'slope',\n",
              " 'aspect',\n",
              " 'age',\n",
              " 'BP']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRVKhwNf7JnP",
        "outputId": "283ded1b-c5c7-437a-eb90-1b73c4e43346"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# specify a kernel/image size to use for the model\n",
        "KERNEL_SIZE = 256\n",
        "\n",
        "# create an EE kernel opject from the kernel size\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)"
      ],
      "metadata": {
        "id": "eBimyLCIl4m6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel.getInfo()"
      ],
      "metadata": {
        "id": "1BFtei8AntAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sizes of the training and evaluation datasets.\n",
        "# based on sizes of exported data and spliting performed earlier\n",
        "# ~20 counties with 300 samples per county = ~6000 samples\n",
        "# ~60% are training, ~25% are testing, ~15% are validation\n",
        "TRAIN_SIZE = 1000\n",
        "TEST_SIZE =  500\n",
        "VAL_SIZE = 500\n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "BUFFER_SIZE = 500#1500 # setting too large will give an Out of Memory (OOM) error\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))"
      ],
      "metadata": {
        "id": "AFbrF9K6mWzA"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURES_DICT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXwrBMNJn9TR",
        "outputId": "579aba68-e3ac-48f3-f3e4-4ccde524341b"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BP': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'age': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'agfast': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'agmed': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'agslo': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'agvfast': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'aspect': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'bgfast': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'bgslo': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'bgvfast': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'croot': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'elevation': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'foliage': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'froot': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'ldist': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'merch': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'otherw': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'pyrome_freq': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'slope': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'snbran': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'snstem': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "    \"\"\"The parsing function.\n",
        "    Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "    Args:\n",
        "    example_proto: a serialized Example.\n",
        "    Returns: \n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "    \"\"\"\n",
        "    return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple_both(inputs):\n",
        "  # cast to int32 for one hot, it stays as original input for some reason \n",
        "  one_hot_inputs = [tf.one_hot(tf.cast(inputs.get(key), tf.int32), CATEGORICAL_DICT.get(key)) for key in CATEGORICAL]\n",
        "  categorical_stacked = tf.stack(one_hot_inputs, axis=-1)\n",
        "\n",
        "  categorical_stacked = tf.squeeze(categorical_stacked) #shape=(256, 256, 10), dtype=tf.float32\n",
        "\n",
        "  # continuous inputs are all features minus categorical bands that have been one hot encoded\n",
        "  continuous_inputs = [inputs.get(key) for key in FEATURES if key not in CATEGORICAL]\n",
        "  continuous_stacked = tf.stack(continuous_inputs, axis=0)\n",
        "  continuous_stacked = tf.transpose(continuous_stacked, [1, 2, 0])#shape=(256, 256, 20), dtype=float32)\n",
        "\n",
        "  # out stack is a tuple of (predictor features, response label)\n",
        "  # predictor features = concat( continuous bands, onehot categorical bands)\n",
        "  out_stack = tf.concat([continuous_stacked[:,:,:len(BANDS)], categorical_stacked],axis=-1), continuous_stacked[:,:,len(BANDS):]#shape=(256, 256, 29), dtype=tf.float32, name=None), TensorSpec(shape=(256, 256, 0)\n",
        "  return out_stack\n",
        "\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "    \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "    Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "    Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "    Returns: \n",
        "    A dtuple of (inputs, outputs).\n",
        "    \"\"\"\n",
        "    inputsList = [inputs.get(key) for key in FEATURES]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    # Convert from CHW to HWC\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "\n",
        "def get_dataset(pattern,training=False):\n",
        "    \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "    Get all the files matching the pattern, parse and convert to tuple.\n",
        "    Args:\n",
        "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "    Returns: \n",
        "    A tf.data.Dataset\n",
        "    \"\"\"\n",
        "    glob = tf.io.gfile.glob(pattern)\n",
        "    dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "    dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "    dataset = dataset.map(to_tuple_both, num_parallel_calls=5)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "AqwjPkenmW4q"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom decoder block to upsample the features in the network\n",
        "# this specific decoder block uses a cov2d -> concat -> conv2d * n -> bilinear upsample\n",
        "def decoder_block(input_tensor, concat_tensor=None, nFilters=512,nConvs=2,i=0,name_prefix=\"decoder_block\"):\n",
        "    deconv = input_tensor\n",
        "    for j in range(nConvs):\n",
        "        deconv = layers.Conv2D(nFilters, 3, activation='relu',\n",
        "                               padding='same',name=f\"{name_prefix}{i}_deconv{j+1}\")(deconv)\n",
        "        deconv = layers.BatchNormalization(name=f\"{name_prefix}{i}_batchnorm{j+1}\")(deconv)\n",
        "        if j == 0:\n",
        "            if concat_tensor is not None:\n",
        "                 deconv = layers.concatenate([deconv,concat_tensor],name=f\"{name_prefix}{i}_concat\")\n",
        "            deconv = layers.Dropout(0.2, seed=0+i,name=f\"{name_prefix}{i}_dropout\")(deconv)\n",
        "    \n",
        "    up = layers.UpSampling2D(interpolation='bilinear',name=f\"{name_prefix}{i}_upsamp\")(deconv)\n",
        "    return up\n",
        "# here we define the network using the VGG-16 encoder \n",
        "# and build our decoder from there\n",
        "# tf_run.final_activation = 'sigmoid'#TODO change this\n",
        "# specify an input tensor with an arbitrary shape for x and y dims\n",
        "# has sample length channels as landsat bands we exported\n",
        "\n",
        "inTensor = layers.Input(shape=[None,None,len(BANDS)+sum(CATEGORICAL_DICt.values())],name=\"input\")#todo change len\n",
        "\n",
        "# grab the vgg-16 encoder and build based off our input tensor\n",
        "vgg16 = keras.applications.VGG19(include_top=False,weights=None,input_tensor=inTensor)\n",
        "\n",
        "# grab the input and output tensors\n",
        "base_in = vgg16.input\n",
        "base_out = vgg16.output\n",
        "\n",
        "# extract the tensors we will use to concatenate our decoders with\n",
        "concat_layers = [\"block5_conv3\",\"block4_conv3\",\"block3_conv3\",\"block2_conv2\",\"block1_conv2\"]\n",
        "concat_tensors = [vgg16.get_layer(layer).output for layer in concat_layers]\n",
        "\n",
        "# define the decoder branch\n",
        "\n",
        "decoder0 = decoder_block(base_out, nFilters=512,nConvs=1,i=0) # center block with no upsampling\n",
        "decoder1 = decoder_block(decoder0, concat_tensor=concat_tensors[0], nFilters=512,nConvs=1,i=2) \n",
        "decoder2 = decoder_block(decoder1, concat_tensor=concat_tensors[1], nFilters=256,nConvs=1,i=3) \n",
        "decoder3 = decoder_block(decoder2, concat_tensor=concat_tensors[2], nFilters=128,nConvs=1,i=4) \n",
        "decoder4 = decoder_block(decoder3, concat_tensor=concat_tensors[3], nFilters=64,nConvs=1,i=5) \n",
        "# concat the final decoder block with the first encoder output\n",
        "# drop out correlated connections in spatial space\n",
        "outBranch = layers.concatenate([decoder4,concat_tensors[4]],name=\"out_block_concat1\")\n",
        "outBranch = layers.SpatialDropout2D(rate=0.2,seed=0,name=\"out_block_spatialdrop\")(outBranch)\n",
        "\n",
        "# perform some additional convolutions before predicting probabilites\n",
        "outBranch = layers.Conv2D(64, 3, activation='relu', \n",
        "                          padding='same',name=\"out_block_conv1\")(outBranch)\n",
        "outBranch = layers.BatchNormalization(name=\"out_block_batchnorm1\")(outBranch)\n",
        "outBranch = layers.Conv2D(64, 3, activation='relu', \n",
        "                          padding='same',name=\"out_block_conv2\")(outBranch)\n",
        "outBranch = layers.BatchNormalization(name=\"out_block_batchnorm2\")(outBranch)\n",
        "# final convolution and softmax activation to get output probabilities\n",
        "# nodes will equal the number of classes\n",
        "outBranch = layers.Conv2D(len(RESPONSE), (1, 1),name='final_conv')(outBranch)#to cahnge len back\n",
        "output = layers.Activation('sigmoid', name=\"final_out\")(outBranch)\n",
        " \n",
        "# declare our model with the inputs from the encoder and outputs from the decoder\n",
        "model = models.Model(inputs=[base_in], outputs=[output],name=\"vgg16-unet\")\n",
        "\n",
        "# define an adaptive learning rate based on training\n",
        "lr_schedule = keras.optimizers.schedules.InverseTimeDecay(\n",
        "  0.001,\n",
        "  decay_steps=500,\n",
        "  decay_rate=1,\n",
        "  staircase=False)\n",
        "\n",
        "# compile the model\n",
        "# uses Adam loss with adaptive learning rate\n",
        "# soft dice loss as opjective function\n",
        "# outputs accuracy, precision, recall, and f1\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr_schedule),\n",
        "              loss='mean_absolute_error',#dice_loss,\n",
        "              metrics=['mean_squared_error',#keras.metrics.categorical_accuracy,\n",
        "                       tf.keras.metrics.RootMeanSquaredError(),\n",
        "                       tf.keras.metrics.MeanAbsoluteError()\n",
        "                       ])\n",
        "\n",
        "# display the model summary to see layers and parameters\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "23RfjOCMdg1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_dataset():\n",
        "    \"\"\"Get the preprocessed training dataset\n",
        "    Returns: \n",
        "    A tf.data.Dataset of training data.\n",
        "    \"\"\"\n",
        "    glob = 'gs://' + BUCKET + '/' + FOLDER + '/' + TRAINING_BASE + '*'\n",
        "    dataset = get_dataset(glob,training=True)\n",
        "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "    return dataset\n",
        "\n",
        "training = get_training_dataset()"
      ],
      "metadata": {
        "id": "RAXjrnLJoyAO"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_testing_dataset():\n",
        "\t\"\"\"Get the preprocessed evaluation dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of evaluation data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + TESTING_BASE + '*'\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.batch(1).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "testing = get_testing_dataset()"
      ],
      "metadata": {
        "id": "I1nDOTXrpACD"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model!!!\n",
        "history = model.fit(x=training,\n",
        "                    epochs=EPOCHS,\n",
        "                    steps_per_epoch=(TRAIN_SIZE // BATCH_SIZE),\n",
        "                    validation_data=testing,\n",
        "                    validation_steps=TEST_SIZE,\n",
        "                    # callbacks=[earlyStopping],\n",
        "                   )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG-1HhYzpIi2",
        "outputId": "dace0c9d-f98a-44ce-dc86-a7210c2f7e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##dev area\n",
        "\n",
        "figuring out shapes\n"
      ],
      "metadata": {
        "id": "1zrsKO8ec1EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BANDS = ['agfast', 'agmed', 'agslo', 'agvfast', 'bgfast', 'bgslo', 'bgvfast',\n",
        "         'croot', 'froot', 'foliage', 'merch', 'otherw', 'snbran', 'snstem',\n",
        "         'ldist', 'elevation', 'slope', 'aspect', 'age']\n",
        "\n",
        "CATEGORICAL = ['pyrome_freq']\n",
        "CATEGORICAL_DICT = {\n",
        "    'pyrome_freq':10\n",
        "}\n",
        "\n",
        "RESPONSE = ['BP']\n",
        "\n",
        "FEATURES = CATEGORICAL + BANDS + RESPONSE\n"
      ],
      "metadata": {
        "id": "W-8QlyeTqZdt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#setup \n",
        "kernal size and shape, feature dict, and default parse_tfrecord"
      ],
      "metadata": {
        "id": "CpgPoivmrcFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "    \"\"\"The parsing function.\n",
        "    Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "    Args:\n",
        "    example_proto: a serialized Example.\n",
        "    Returns: \n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "    \"\"\"\n",
        "    return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "# specify a kernel/image size to use for the model\n",
        "KERNEL_SIZE = 256\n",
        "\n",
        "# create an EE kernel opject from the kernel size\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))"
      ],
      "metadata": {
        "id": "ZfFCVgaCrFQf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#testing \n",
        "* onehot parsing \n",
        "* regular parseing\n",
        "* onehot and regular parseing"
      ],
      "metadata": {
        "id": "R4BzEbQLrmiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.gen_array_ops import one_hot\n",
        "\n",
        "singleGlob = \"gs://landfire/lucas/pools2fire_tf/testing_patches_v11_i17.tfrecord.gz\"\n",
        "glob = tf.io.gfile.glob(singleGlob)\n",
        "dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP').map(parse_tfrecord)\n",
        "\n",
        "def to_tuple_onehot(inputs):\n",
        "  # cast to int32 for one hot, it stays as input(float32) without needing to be recast for some reason \n",
        "  one_hot_int = [tf.one_hot(tf.cast(inputs.get(key), tf.int32), CATEGORICAL_DICT.get(key)) for key in CATEGORICAL]\n",
        "  \n",
        "  stacked = tf.stack(one_hot_int,axis=-1)\n",
        "  print('onehot b4 squze',stacked)\n",
        "  # squeeze to remove extra dimention\n",
        "  return tf.squeeze(stacked)\n",
        "\n",
        "def to_tuple_reg(inputs):\n",
        "    \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "    Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "    Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "    Returns: \n",
        "    A dtuple of (inputs, outputs).\n",
        "    \"\"\"\n",
        "    inputsList = [inputs.get(key) for key in FEATURES  if key not in CATEGORICAL]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    print('CHW',stacked)\n",
        "    # Convert from CHW to HWC\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    print('HWC',stacked)\n",
        "    return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "def to_tuple_onehot_reg(inputs):\n",
        "  # cast to int32 for one hot, it stays as original input for some reason \n",
        "  one_hot_inputs = [tf.one_hot(tf.cast(inputs.get(key), tf.int32), CATEGORICAL_DICT.get(key)) for key in CATEGORICAL]\n",
        "  categorical_stacked = tf.stack(one_hot_inputs, axis=-1)\n",
        "\n",
        "  categorical_stacked = tf.squeeze(categorical_stacked) #shape=(256, 256, 10), dtype=tf.float32\n",
        "\n",
        "  # continuous inputs are all features minus categorical bands that have been one hot encoded\n",
        "  continuous_inputs = [inputs.get(key) for key in FEATURES if key not in CATEGORICAL]\n",
        "  continuous_stacked = tf.stack(continuous_inputs, axis=0)\n",
        "  continuous_stacked = tf.transpose(continuous_stacked, [1, 2, 0])#shape=(256, 256, 20), dtype=float32)\n",
        "\n",
        "  # out stack is a tuple of (predictor features, response label)\n",
        "  # predictor features = concat( continuous bands, onehot categorical bands)\n",
        "  out_stack = tf.concat([continuous_stacked[:,:,:len(BANDS)], categorical_stacked],axis=-1), continuous_stacked[:,:,len(BANDS):]#shape=(256, 256, 29), dtype=tf.float32, name=None), TensorSpec(shape=(256, 256, 0)\n",
        "  return out_stack\n",
        "\n",
        "inputs = dataset\n",
        "inputsListCate =  dataset.map(to_tuple_onehot)\n",
        "\n",
        "print(inputsListCate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKW3pmuX-bAZ",
        "outputId": "e98176c3-063f-4bb7-b023-37884d46cfdf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "onehot b4 squze Tensor(\"stack:0\", shape=(1, 256, 256, 10), dtype=float32)\n",
            "<MapDataset element_spec=TensorSpec(shape=(256, 256, 10), dtype=tf.float32, name=None)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first parsed record to check.\n",
        "from pprint import pprint\n",
        "pprint(iter(inputsListCate).next())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArTASQTa2OIG",
        "outputId": "caf1c470-03c2-4fba-c018-d7988c0ea49f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(256, 256, 29), dtype=float32, numpy=\n",
            "array([[[ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        ...,\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ]],\n",
            "\n",
            "       [[ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        ...,\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ]],\n",
            "\n",
            "       [[ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        ...,\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.2893,  0.    ,  3.0792, ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.2893,  0.    ,  3.0792, ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.2893,  0.    ,  3.0792, ...,  0.    ,  0.    ,  0.    ],\n",
            "        ...,\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ]],\n",
            "\n",
            "       [[ 0.2893,  0.    ,  3.0792, ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.2893,  0.    ,  3.0792, ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.2893,  0.    ,  3.0792, ...,  0.    ,  0.    ,  0.    ],\n",
            "        ...,\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ]],\n",
            "\n",
            "       [[ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        ...,\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ],\n",
            "        [ 0.    ,  0.    , 10.835 , ...,  0.    ,  0.    ,  0.    ]]],\n",
            "      dtype=float32)>,\n",
            " <tf.Tensor: shape=(256, 256, 0), dtype=float32, numpy=array([], shape=(256, 256, 0), dtype=float32)>)\n"
          ]
        }
      ]
    }
  ]
}