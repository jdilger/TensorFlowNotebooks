{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "onehot_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIy/MTVhwgS/B42X4b3VwN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdilger/TensorFlowNotebooks/blob/master/onehot_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SyY3WjXRjoYR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = [[0, 1, 2],\n",
        "           [4, 5, 1],\n",
        "           [0, 1, 2]]\n",
        "depth = 6\n",
        "tf.one_hot(indices, depth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMSa2k9GjsFG",
        "outputId": "f6cf22f6-e693-47dc-daa9-0329a99355fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3, 6), dtype=float32, numpy=\n",
              "array([[[1., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 1.],\n",
              "        [0., 1., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[1., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0.]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import ee\n",
        "try:\n",
        "  ee.Initialize()\n",
        "except:\n",
        "  ee.Authenticate()\n",
        "  ee.Initialize()\n",
        "\n",
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "0oD-9HaNldc3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify cloud storage bucket to save data too\n",
        "BUCKET = 'landfire'\n",
        "\n",
        "# Specify names locations for outputs in Cloud Storage. \n",
        "FOLDER = 'lucas/pools2fire_tf'\n",
        "TRAINING_BASE = 'training_patches_v11'\n",
        "TESTING_BASE = 'testing_patches_v11'\n",
        "VAL_BASE = 'val_patches_v4'\n",
        "\n",
        "BANDS = ['agfast', 'agmed', 'agslo', 'agvfast', 'bgfast', 'bgslo', 'bgvfast',\n",
        "         'croot', 'froot', 'foliage', 'merch', 'otherw', 'snbran', 'snstem',\n",
        "         'ldist', 'elevation', 'slope', 'aspect', 'age']\n",
        "CATEGORICAL = ['pyrome_freq']\n",
        "CATEGORICAL_DICt = {\n",
        "    'pyrome_freq':10\n",
        "}\n",
        "RESPONSE = ['BP']\n",
        "FEATURES = CATEGORICAL + BANDS + RESPONSE\n",
        "FEATURES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TemJ8za5lmlk",
        "outputId": "2c932f30-166a-4f65-8050-49c5d8fe9efc"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pyrome_freq',\n",
              " 'agfast',\n",
              " 'agmed',\n",
              " 'agslo',\n",
              " 'agvfast',\n",
              " 'bgfast',\n",
              " 'bgslo',\n",
              " 'bgvfast',\n",
              " 'croot',\n",
              " 'froot',\n",
              " 'foliage',\n",
              " 'merch',\n",
              " 'otherw',\n",
              " 'snbran',\n",
              " 'snstem',\n",
              " 'ldist',\n",
              " 'elevation',\n",
              " 'slope',\n",
              " 'aspect',\n",
              " 'age',\n",
              " 'BP']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRVKhwNf7JnP",
        "outputId": "283ded1b-c5c7-437a-eb90-1b73c4e43346"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# specify a kernel/image size to use for the model\n",
        "KERNEL_SIZE = 256\n",
        "\n",
        "# create an EE kernel opject from the kernel size\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)"
      ],
      "metadata": {
        "id": "eBimyLCIl4m6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel.getInfo()"
      ],
      "metadata": {
        "id": "1BFtei8AntAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sizes of the training and evaluation datasets.\n",
        "# based on sizes of exported data and spliting performed earlier\n",
        "# ~20 counties with 300 samples per county = ~6000 samples\n",
        "# ~60% are training, ~25% are testing, ~15% are validation\n",
        "TRAIN_SIZE = 3 \n",
        "TEST_SIZE =  3\n",
        "VAL_SIZE = 3\n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 3\n",
        "BUFFER_SIZE = 1#1500 # setting too large will give an Out of Memory (OOM) error\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))"
      ],
      "metadata": {
        "id": "AFbrF9K6mWzA"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURES_DICT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXwrBMNJn9TR",
        "outputId": "d58dfc0d-5c16-424f-c9d8-8994bb800366"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BP': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'age': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'agfast': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'agmed': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'agslo': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'agvfast': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'aspect': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'bgfast': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'bgslo': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'bgvfast': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'croot': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'elevation': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'foliage': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'froot': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'ldist': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'merch': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'otherw': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'pyrome_freq': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'slope': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'snbran': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None),\n",
              " 'snstem': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "    \"\"\"The parsing function.\n",
        "    Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "    Args:\n",
        "    example_proto: a serialized Example.\n",
        "    Returns: \n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "    \"\"\"\n",
        "    return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple_both(inputs):\n",
        "  # cast to int32 for one hot, it stays as original input for some reason \n",
        "  one_hot_int = [tf.one_hot(tf.cast(inputs.get(key), tf.int32), CATEGORICAL_DICt.get(key)) for key in CATEGORICAL]\n",
        "  stacked = tf.stack(one_hot_int,axis=-1)\n",
        "  \n",
        "  stacked = tf.squeeze(stacked)\n",
        "\n",
        "  # contin\n",
        "  conInputsList = [inputs.get(key) for key in FEATURES if key not in CATEGORICAL]\n",
        "  conStacked = tf.stack(conInputsList, axis=0)\n",
        "  conStacked = tf.transpose(conStacked, [1, 2, 0])\n",
        "\n",
        "  # stack and split off predictor\n",
        "  outStack = tf.concat([conStacked[:,:,:len(BANDS)], stacked],axis=-1), conStacked[:,:,len(BANDS):]\n",
        "  return outStack\n",
        "\n",
        "def to_tuple(inputs):\n",
        "    \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "    Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "    Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "    Returns: \n",
        "    A dtuple of (inputs, outputs).\n",
        "    \"\"\"\n",
        "    inputsList = [inputs.get(key) for key in FEATURES]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    # Convert from CHW to HWC\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "def to_tuple_cat(inputs):\n",
        "    \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "    Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "    Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "    Returns: \n",
        "    A dtuple of (inputs, outputs).\n",
        "    \"\"\"\n",
        "    inputsList = [tf.one_hot(inputs.get(key), CATEGORICAL_DICt.get(key)) for key in CATEGORICAL]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    # Convert from CHW to HWC\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked[:,:,:len(CATEGORICAL)], stacked[:,:,len(CATEGORICAL):]\n",
        "\n",
        "def get_dataset(pattern,training=False):\n",
        "    \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "    Get all the files matching the pattern, parse and convert to tuple.\n",
        "    Args:\n",
        "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "    Returns: \n",
        "    A tf.data.Dataset\n",
        "    \"\"\"\n",
        "    glob = tf.io.gfile.glob(pattern)\n",
        "    dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "    dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "    dataset = dataset.map(to_tuple_both, num_parallel_calls=5)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "AqwjPkenmW4q"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom decoder block to upsample the features in the network\n",
        "# this specific decoder block uses a cov2d -> concat -> conv2d * n -> bilinear upsample\n",
        "def decoder_block(input_tensor, concat_tensor=None, nFilters=512,nConvs=2,i=0,name_prefix=\"decoder_block\"):\n",
        "    deconv = input_tensor\n",
        "    for j in range(nConvs):\n",
        "        deconv = layers.Conv2D(nFilters, 3, activation='relu',\n",
        "                               padding='same',name=f\"{name_prefix}{i}_deconv{j+1}\")(deconv)\n",
        "        deconv = layers.BatchNormalization(name=f\"{name_prefix}{i}_batchnorm{j+1}\")(deconv)\n",
        "        if j == 0:\n",
        "            if concat_tensor is not None:\n",
        "                 deconv = layers.concatenate([deconv,concat_tensor],name=f\"{name_prefix}{i}_concat\")\n",
        "            deconv = layers.Dropout(0.2, seed=0+i,name=f\"{name_prefix}{i}_dropout\")(deconv)\n",
        "    \n",
        "    up = layers.UpSampling2D(interpolation='bilinear',name=f\"{name_prefix}{i}_upsamp\")(deconv)\n",
        "    return up\n",
        "# here we define the network using the VGG-16 encoder \n",
        "# and build our decoder from there\n",
        "# tf_run.final_activation = 'sigmoid'#TODO change this\n",
        "# specify an input tensor with an arbitrary shape for x and y dims\n",
        "# has sample length channels as landsat bands we exported\n",
        "\n",
        "inTensor = layers.Input(shape=[None,None,len(BANDS)+sum(CATEGORICAL_DICt.values())],name=\"input\")#todo change len\n",
        "\n",
        "# grab the vgg-16 encoder and build based off our input tensor\n",
        "vgg16 = keras.applications.VGG19(include_top=False,weights=None,input_tensor=inTensor)\n",
        "\n",
        "# grab the input and output tensors\n",
        "base_in = vgg16.input\n",
        "base_out = vgg16.output\n",
        "\n",
        "# extract the tensors we will use to concatenate our decoders with\n",
        "concat_layers = [\"block5_conv3\",\"block4_conv3\",\"block3_conv3\",\"block2_conv2\",\"block1_conv2\"]\n",
        "concat_tensors = [vgg16.get_layer(layer).output for layer in concat_layers]\n",
        "\n",
        "# define the decoder branch\n",
        "\n",
        "decoder0 = decoder_block(base_out, nFilters=512,nConvs=1,i=0) # center block with no upsampling\n",
        "decoder1 = decoder_block(decoder0, concat_tensor=concat_tensors[0], nFilters=512,nConvs=1,i=2) \n",
        "decoder2 = decoder_block(decoder1, concat_tensor=concat_tensors[1], nFilters=256,nConvs=1,i=3) \n",
        "decoder3 = decoder_block(decoder2, concat_tensor=concat_tensors[2], nFilters=128,nConvs=1,i=4) \n",
        "decoder4 = decoder_block(decoder3, concat_tensor=concat_tensors[3], nFilters=64,nConvs=1,i=5) \n",
        "# concat the final decoder block with the first encoder output\n",
        "# drop out correlated connections in spatial space\n",
        "outBranch = layers.concatenate([decoder4,concat_tensors[4]],name=\"out_block_concat1\")\n",
        "outBranch = layers.SpatialDropout2D(rate=0.2,seed=0,name=\"out_block_spatialdrop\")(outBranch)\n",
        "\n",
        "# perform some additional convolutions before predicting probabilites\n",
        "outBranch = layers.Conv2D(64, 3, activation='relu', \n",
        "                          padding='same',name=\"out_block_conv1\")(outBranch)\n",
        "outBranch = layers.BatchNormalization(name=\"out_block_batchnorm1\")(outBranch)\n",
        "outBranch = layers.Conv2D(64, 3, activation='relu', \n",
        "                          padding='same',name=\"out_block_conv2\")(outBranch)\n",
        "outBranch = layers.BatchNormalization(name=\"out_block_batchnorm2\")(outBranch)\n",
        "# final convolution and softmax activation to get output probabilities\n",
        "# nodes will equal the number of classes\n",
        "outBranch = layers.Conv2D(len(RESPONSE), (1, 1),name='final_conv')(outBranch)#to cahnge len back\n",
        "output = layers.Activation('sigmoid', name=\"final_out\")(outBranch)\n",
        " \n",
        "# declare our model with the inputs from the encoder and outputs from the decoder\n",
        "model = models.Model(inputs=[base_in], outputs=[output],name=\"vgg16-unet\")\n",
        "\n",
        "# define an adaptive learning rate based on training\n",
        "lr_schedule = keras.optimizers.schedules.InverseTimeDecay(\n",
        "  0.001,\n",
        "  decay_steps=500,\n",
        "  decay_rate=1,\n",
        "  staircase=False)\n",
        "\n",
        "# compile the model\n",
        "# uses Adam loss with adaptive learning rate\n",
        "# soft dice loss as opjective function\n",
        "# outputs accuracy, precision, recall, and f1\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr_schedule),\n",
        "              loss='mean_absolute_error',#dice_loss,\n",
        "              metrics=['mean_squared_error',#keras.metrics.categorical_accuracy,\n",
        "                       tf.keras.metrics.RootMeanSquaredError(),\n",
        "                       tf.keras.metrics.MeanAbsoluteError()\n",
        "                       ])\n",
        "\n",
        "# display the model summary to see layers and parameters\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23RfjOCMdg1d",
        "outputId": "b4e54d65-395a-49ac-bbe9-01458ed41160"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16-unet\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, None, None,  0           []                               \n",
            "                                 29)]                                                             \n",
            "                                                                                                  \n",
            " block1_conv1 (Conv2D)          (None, None, None,   16768       ['input[0][0]']                  \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " block1_conv2 (Conv2D)          (None, None, None,   36928       ['block1_conv1[0][0]']           \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " block1_pool (MaxPooling2D)     (None, None, None,   0           ['block1_conv2[0][0]']           \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " block2_conv1 (Conv2D)          (None, None, None,   73856       ['block1_pool[0][0]']            \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " block2_conv2 (Conv2D)          (None, None, None,   147584      ['block2_conv1[0][0]']           \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " block2_pool (MaxPooling2D)     (None, None, None,   0           ['block2_conv2[0][0]']           \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " block3_conv1 (Conv2D)          (None, None, None,   295168      ['block2_pool[0][0]']            \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " block3_conv2 (Conv2D)          (None, None, None,   590080      ['block3_conv1[0][0]']           \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " block3_conv3 (Conv2D)          (None, None, None,   590080      ['block3_conv2[0][0]']           \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " block3_conv4 (Conv2D)          (None, None, None,   590080      ['block3_conv3[0][0]']           \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " block3_pool (MaxPooling2D)     (None, None, None,   0           ['block3_conv4[0][0]']           \n",
            "                                256)                                                              \n",
            "                                                                                                  \n",
            " block4_conv1 (Conv2D)          (None, None, None,   1180160     ['block3_pool[0][0]']            \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " block4_conv2 (Conv2D)          (None, None, None,   2359808     ['block4_conv1[0][0]']           \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " block4_conv3 (Conv2D)          (None, None, None,   2359808     ['block4_conv2[0][0]']           \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " block4_conv4 (Conv2D)          (None, None, None,   2359808     ['block4_conv3[0][0]']           \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " block4_pool (MaxPooling2D)     (None, None, None,   0           ['block4_conv4[0][0]']           \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " block5_conv1 (Conv2D)          (None, None, None,   2359808     ['block4_pool[0][0]']            \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " block5_conv2 (Conv2D)          (None, None, None,   2359808     ['block5_conv1[0][0]']           \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " block5_conv3 (Conv2D)          (None, None, None,   2359808     ['block5_conv2[0][0]']           \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " block5_conv4 (Conv2D)          (None, None, None,   2359808     ['block5_conv3[0][0]']           \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " block5_pool (MaxPooling2D)     (None, None, None,   0           ['block5_conv4[0][0]']           \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " decoder_block0_deconv1 (Conv2D  (None, None, None,   2359808    ['block5_pool[0][0]']            \n",
            " )                              512)                                                              \n",
            "                                                                                                  \n",
            " decoder_block0_batchnorm1 (Bat  (None, None, None,   2048       ['decoder_block0_deconv1[0][0]'] \n",
            " chNormalization)               512)                                                              \n",
            "                                                                                                  \n",
            " decoder_block0_dropout (Dropou  (None, None, None,   0          ['decoder_block0_batchnorm1[0][0]\n",
            " t)                             512)                             ']                               \n",
            "                                                                                                  \n",
            " decoder_block0_upsamp (UpSampl  (None, None, None,   0          ['decoder_block0_dropout[0][0]'] \n",
            " ing2D)                         512)                                                              \n",
            "                                                                                                  \n",
            " decoder_block2_deconv1 (Conv2D  (None, None, None,   2359808    ['decoder_block0_upsamp[0][0]']  \n",
            " )                              512)                                                              \n",
            "                                                                                                  \n",
            " decoder_block2_batchnorm1 (Bat  (None, None, None,   2048       ['decoder_block2_deconv1[0][0]'] \n",
            " chNormalization)               512)                                                              \n",
            "                                                                                                  \n",
            " decoder_block2_concat (Concate  (None, None, None,   0          ['decoder_block2_batchnorm1[0][0]\n",
            " nate)                          1024)                            ',                               \n",
            "                                                                  'block5_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " decoder_block2_dropout (Dropou  (None, None, None,   0          ['decoder_block2_concat[0][0]']  \n",
            " t)                             1024)                                                             \n",
            "                                                                                                  \n",
            " decoder_block2_upsamp (UpSampl  (None, None, None,   0          ['decoder_block2_dropout[0][0]'] \n",
            " ing2D)                         1024)                                                             \n",
            "                                                                                                  \n",
            " decoder_block3_deconv1 (Conv2D  (None, None, None,   2359552    ['decoder_block2_upsamp[0][0]']  \n",
            " )                              256)                                                              \n",
            "                                                                                                  \n",
            " decoder_block3_batchnorm1 (Bat  (None, None, None,   1024       ['decoder_block3_deconv1[0][0]'] \n",
            " chNormalization)               256)                                                              \n",
            "                                                                                                  \n",
            " decoder_block3_concat (Concate  (None, None, None,   0          ['decoder_block3_batchnorm1[0][0]\n",
            " nate)                          768)                             ',                               \n",
            "                                                                  'block4_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " decoder_block3_dropout (Dropou  (None, None, None,   0          ['decoder_block3_concat[0][0]']  \n",
            " t)                             768)                                                              \n",
            "                                                                                                  \n",
            " decoder_block3_upsamp (UpSampl  (None, None, None,   0          ['decoder_block3_dropout[0][0]'] \n",
            " ing2D)                         768)                                                              \n",
            "                                                                                                  \n",
            " decoder_block4_deconv1 (Conv2D  (None, None, None,   884864     ['decoder_block3_upsamp[0][0]']  \n",
            " )                              128)                                                              \n",
            "                                                                                                  \n",
            " decoder_block4_batchnorm1 (Bat  (None, None, None,   512        ['decoder_block4_deconv1[0][0]'] \n",
            " chNormalization)               128)                                                              \n",
            "                                                                                                  \n",
            " decoder_block4_concat (Concate  (None, None, None,   0          ['decoder_block4_batchnorm1[0][0]\n",
            " nate)                          384)                             ',                               \n",
            "                                                                  'block3_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " decoder_block4_dropout (Dropou  (None, None, None,   0          ['decoder_block4_concat[0][0]']  \n",
            " t)                             384)                                                              \n",
            "                                                                                                  \n",
            " decoder_block4_upsamp (UpSampl  (None, None, None,   0          ['decoder_block4_dropout[0][0]'] \n",
            " ing2D)                         384)                                                              \n",
            "                                                                                                  \n",
            " decoder_block5_deconv1 (Conv2D  (None, None, None,   221248     ['decoder_block4_upsamp[0][0]']  \n",
            " )                              64)                                                               \n",
            "                                                                                                  \n",
            " decoder_block5_batchnorm1 (Bat  (None, None, None,   256        ['decoder_block5_deconv1[0][0]'] \n",
            " chNormalization)               64)                                                               \n",
            "                                                                                                  \n",
            " decoder_block5_concat (Concate  (None, None, None,   0          ['decoder_block5_batchnorm1[0][0]\n",
            " nate)                          192)                             ',                               \n",
            "                                                                  'block2_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " decoder_block5_dropout (Dropou  (None, None, None,   0          ['decoder_block5_concat[0][0]']  \n",
            " t)                             192)                                                              \n",
            "                                                                                                  \n",
            " decoder_block5_upsamp (UpSampl  (None, None, None,   0          ['decoder_block5_dropout[0][0]'] \n",
            " ing2D)                         192)                                                              \n",
            "                                                                                                  \n",
            " out_block_concat1 (Concatenate  (None, None, None,   0          ['decoder_block5_upsamp[0][0]',  \n",
            " )                              256)                              'block1_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " out_block_spatialdrop (Spatial  (None, None, None,   0          ['out_block_concat1[0][0]']      \n",
            " Dropout2D)                     256)                                                              \n",
            "                                                                                                  \n",
            " out_block_conv1 (Conv2D)       (None, None, None,   147520      ['out_block_spatialdrop[0][0]']  \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " out_block_batchnorm1 (BatchNor  (None, None, None,   256        ['out_block_conv1[0][0]']        \n",
            " malization)                    64)                                                               \n",
            "                                                                                                  \n",
            " out_block_conv2 (Conv2D)       (None, None, None,   36928       ['out_block_batchnorm1[0][0]']   \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " out_block_batchnorm2 (BatchNor  (None, None, None,   256        ['out_block_conv2[0][0]']        \n",
            " malization)                    64)                                                               \n",
            "                                                                                                  \n",
            " final_conv (Conv2D)            (None, None, None,   65          ['out_block_batchnorm2[0][0]']   \n",
            "                                1)                                                                \n",
            "                                                                                                  \n",
            " final_out (Activation)         (None, None, None,   0           ['final_conv[0][0]']             \n",
            "                                1)                                                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 28,415,553\n",
            "Trainable params: 28,412,353\n",
            "Non-trainable params: 3,200\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_dataset():\n",
        "    \"\"\"Get the preprocessed training dataset\n",
        "    Returns: \n",
        "    A tf.data.Dataset of training data.\n",
        "    \"\"\"\n",
        "    glob = 'gs://' + BUCKET + '/' + FOLDER + '/' + TRAINING_BASE + '*'\n",
        "    dataset = get_dataset(glob,training=True)\n",
        "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "    return dataset\n",
        "\n",
        "training = get_training_dataset()"
      ],
      "metadata": {
        "id": "RAXjrnLJoyAO"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_testing_dataset():\n",
        "\t\"\"\"Get the preprocessed evaluation dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of evaluation data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + TESTING_BASE + '*'\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.batch(1).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "testing = get_testing_dataset()"
      ],
      "metadata": {
        "id": "I1nDOTXrpACD"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model!!!\n",
        "history = model.fit(x=training,\n",
        "                    epochs=EPOCHS,\n",
        "                    steps_per_epoch=(TRAIN_SIZE // BATCH_SIZE),\n",
        "                    validation_data=testing,\n",
        "                    validation_steps=TEST_SIZE,\n",
        "                    # callbacks=[earlyStopping],\n",
        "                   )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG-1HhYzpIi2",
        "outputId": "fbd1272b-71ba-470c-c6a2-527e887895c4"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "3/3 [==============================] - 33s 10s/step - loss: 0.4637 - mean_squared_error: 0.2820 - root_mean_squared_error: 0.5310 - mean_absolute_error: 0.4637 - val_loss: 0.1438 - val_mean_squared_error: 0.1187 - val_root_mean_squared_error: 0.3445 - val_mean_absolute_error: 0.1438\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 26s 10s/step - loss: 0.3821 - mean_squared_error: 0.2056 - root_mean_squared_error: 0.4535 - mean_absolute_error: 0.3821 - val_loss: 0.1179 - val_mean_squared_error: 0.0886 - val_root_mean_squared_error: 0.2977 - val_mean_absolute_error: 0.1179\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 27s 10s/step - loss: 0.3084 - mean_squared_error: 0.1707 - root_mean_squared_error: 0.4131 - mean_absolute_error: 0.3084 - val_loss: 0.1382 - val_mean_squared_error: 0.1126 - val_root_mean_squared_error: 0.3356 - val_mean_absolute_error: 0.1382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##dev area\n",
        "\n",
        "figuring out shapes\n"
      ],
      "metadata": {
        "id": "1zrsKO8ec1EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " pattern = 'gs://' + BUCKET + '/' + FOLDER + '/' + TRAINING_BASE + '*'\n",
        " glob = tf.io.gfile.glob(pattern)\n",
        " dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        " print(len(glob))\n",
        " dataset = dataset.map(parse_tfrecord).map(to_tuple_both)\n",
        "\n",
        "#  parsed = tf.io.parse_single_example(dataset, FEATURES_DICT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWvGnr14oeZy",
        "outputId": "43280f77-072f-42f7-e7be-dce5583f52b9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.gen_array_ops import one_hot\n",
        "# for key in CATEGORICAL: print(CATEGORICAL_DICt[key])\n",
        "singleGlob = \"gs://landfire/lucas/pools2fire_tf/testing_patches_v11_i17.tfrecord.gz\"\n",
        "glob = tf.io.gfile.glob(singleGlob)\n",
        "dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP').map(parse_tfrecord)\n",
        "\n",
        "def ok(inputs):\n",
        "  # cast to int32 for one hot, it stays as original input for some reason \n",
        "  one_hot_int = [tf.one_hot(tf.cast(inputs.get(key), tf.int32), CATEGORICAL_DICt.get(key)) for key in CATEGORICAL]\n",
        "\n",
        "  stacked = tf.stack(one_hot_int,axis=-1)\n",
        "  # squeeze to remove extra dimention\n",
        "  return tf.squeeze(stacked)\n",
        "def reg(inputs):\n",
        "    \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "    Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "    Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "    Returns: \n",
        "    A dtuple of (inputs, outputs).\n",
        "    \"\"\"\n",
        "    inputsList = [inputs.get(key) for key in FEATURES]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    print('CHW',stacked)\n",
        "    # Convert from CHW to HWC\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "def okidoki(inputs):\n",
        "  # cast to int32 for one hot, it stays as original input for some reason \n",
        "  one_hot_int = [tf.one_hot(tf.cast(inputs.get(key), tf.int32), CATEGORICAL_DICt.get(key)) for key in CATEGORICAL]\n",
        "  stacked = tf.stack(one_hot_int,axis=-1)\n",
        "\n",
        "  stacked = tf.squeeze(stacked)\n",
        "\n",
        "  # contin\n",
        "  conInputsList = [inputs.get(key) for key in BANDS if key not in CATEGORICAL]\n",
        "  conStacked = tf.stack(conInputsList, axis=0)\n",
        "  conStacked = tf.transpose(conStacked, [1, 2, 0])\n",
        "\n",
        "  # stack and split off predictor tf.concat([,stacked], axis-1)\n",
        "  outStack = tf.concat([conStacked[:,:,:len(BANDS)], stacked],axis=-1), conStacked[:,:,len(BANDS):]\n",
        "  return outStack\n",
        "inputs = dataset\n",
        "inputsListCate =  dataset.map(okidoki)\n",
        "# dataset\n",
        "print(inputsListCate)\n",
        "\n",
        "\n",
        "\n",
        "# inputsListCon =  dataset.map(to_tuple)\n",
        "# inputsListCon\n",
        "# shape=(256, 256, 19)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKW3pmuX-bAZ",
        "outputId": "b2e40fdb-6398-4996-827f-91af7b561a4a"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MapDataset element_spec=(TensorSpec(shape=(256, 256, 29), dtype=tf.float32, name=None), TensorSpec(shape=(256, 256, 0), dtype=tf.float32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [[1],[2],[3]]\n",
        "v = 1\n",
        "print(a[:v])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxcUai1rfSWz",
        "outputId": "e0c16f85-b0c5-4b9a-fb2b-213bd91b11a2"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first parsed record to check.\n",
        "from pprint import pprint\n",
        "pprint(iter(inputsListCate).next())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArTASQTa2OIG",
        "outputId": "82b12fa4-9f8c-4780-87ab-8264afe276c4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Tensor: shape=(256, 256, 31), dtype=float32, numpy=\n",
            "array([[[1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        ...,\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00]],\n",
            "\n",
            "       [[1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        ...,\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00]],\n",
            "\n",
            "       [[1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        ...,\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[1.913e+03, 2.893e-01, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 2.893e-01, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 2.893e-01, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        ...,\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00]],\n",
            "\n",
            "       [[1.913e+03, 2.893e-01, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 2.893e-01, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 2.893e-01, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        ...,\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00]],\n",
            "\n",
            "       [[1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        ...,\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00],\n",
            "        [1.913e+03, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
            "         0.000e+00]]], dtype=float32)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_dataset():\n",
        "    \"\"\"Get the preprocessed training dataset\n",
        "    Returns: \n",
        "    A tf.data.Dataset of training data.\n",
        "    \"\"\"\n",
        "    glob = 'gs://' + tf_run.bucket + '/' + tf_run.folder + '/' + tf_sample.training + '*'\n",
        "    dataset = get_dataset(glob,training=True)\n",
        "    dataset = dataset.shuffle(tf_run.buffer_size).batch(tf_run.batch_size).repeat()\n",
        "    return dataset\n",
        "\n",
        "training = get_training_dataset()"
      ],
      "metadata": {
        "id": "pYW2vZ3xnVPC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}