{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exportSamplesUNET.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJ4kW1pEhwP",
        "colab_type": "text"
      },
      "source": [
        "# Setup software libraries\n",
        "\n",
        "Install needed libraries to the notebook VM.  Authenticate as necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neIa46CpciXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D6ArFWrckmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Earth Engine install to notebook VM.\n",
        "!pip install earthengine-api"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jat01FEoUMqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RnZzcYhcpsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorflow setup.\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKs6HuxOzjMl",
        "colab_type": "text"
      },
      "source": [
        "## Specify your Cloud Storage Bucket\n",
        "You must have write access to a bucket to run this demo!  To run it read-only, use the demo bucket below, but note that writes to this bucket will not work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obDDH1eDzsch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# INSERT YOUR BUCKET HERE:\n",
        "BUCKET = 'ee-tf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7f_9BvcnbOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize inputs\n",
        "def basicNorm(img):\n",
        "  img = img.toFloat();\n",
        "  imgStd = img.reduceRegion(reducer=ee.Reducer.stdDev(),scale=0.5,maxPixels=1e13);\n",
        "  imgMinMax = img.reduceRegion(reducer=ee.Reducer.minMax(),scale=0.5,maxPixels=1e13);\n",
        "  \n",
        "  imgMin = ee.Image.constant(imgMinMax.get(imgMinMax.keys().get(1)));\n",
        "  imgMax = ee.Image.constant(imgMinMax.get(imgMinMax.keys().get(0)));\n",
        "  \n",
        "  normImg = img.subtract(imgMin).divide(imgMax.subtract(imgMin));\n",
        "  return normImg.toFloat()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmfKLl9XcnGJ",
        "colab_type": "text"
      },
      "source": [
        "## Set other global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psz7wJKalaoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify names locations for outputs in Cloud Storage. \n",
        "FOLDER = 'tahoe-ogfw-03112020-array-256'\n",
        "TRAINING_BASE = 'Training_tahoe'\n",
        "EVAL_BASE = 'Eval_tahoe'\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "BANDS = ['R','G','B','NIR','L','O','ND']\n",
        "RESPONSE = 'class'\n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgoDc7Hilfc4",
        "colab_type": "text"
      },
      "source": [
        "# Imagery\n",
        "\n",
        "Gather and setup the imagery to use for inputs (predictors).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IlgXu-vcUEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uas = ee.Image(\"projects/sig-ee/PC271-AquaticVegetation/tahoeMosaic_mask\").select(['R','G','B','nir'],['R','G','B','NIR'])\n",
        "# // CIE1931 Luminance we'll use as greyscale model inputs outside the model's \"fovea\"\n",
        "uas_l =uas.select('R').multiply(0.2126).add(uas.select('G').multiply(0.7152).add(uas.select('B').multiply(0.0722))).rename('L');\n",
        "ndvi = uas.normalizedDifference(['NIR','R']).rename('ND')\n",
        "# dpth = ee.Image(\"projects/sig-ee/PC271-AquaticVegetation/dpth_lkth\").rename('dpth')\n",
        "dr40 = ee.Image(\"projects/sig-ee/PC271-AquaticVegetation/DR_40_grn_int_lkth_null_upadted\").rename('O')\n",
        "# rug = ee.Image(\"projects/sig-ee/PC271-AquaticVegetation/rugosity\").rename('rug')\n",
        "image = [basicNorm(uas),basicNorm(uas_l),basicNorm(dr40),basicNorm(ndvi)]\n",
        "image = ee.Image.cat(image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHznnctkJsZJ",
        "colab_type": "text"
      },
      "source": [
        "Prepare the response (what we want to predict).  Combine plant/no plant data, convert to image for sampling. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0wHDyxVirec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add1(f):\n",
        "  return f.set('class',1.0)\n",
        "def add0(f):\n",
        "  return f.set('class',0.0)\n",
        "plants = ee.FeatureCollection('projects/sig-ee/PC271-AquaticVegetation/SamplePloy/PlantSites').map(add1)\n",
        "no_plants =ee.FeatureCollection('projects/sig-ee/PC271-AquaticVegetation/SamplePloy/NoPlantSites_12172019').map(add0)\n",
        "samples = plants.merge(no_plants).randomColumn()\n",
        "\n",
        "sampleImg = samples.reduceToImage(['class'],ee.Reducer.mean()).toFloat().rename('class')\n",
        "image = ee.Image.cat([image,sampleImg])\n",
        "# print(image.getInfo()['bands'])\n",
        "# print(image.bandNames().getInfo())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCznpe-FvGRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = plants.merge(no_plants).randomColumn()\n",
        "trainingSamples = samples.filter(ee.Filter.lt('random', 0.7))\n",
        "evalSamples = samples.filter(ee.Filter.gte('random', 0.7))\n",
        "# print('Eval NP',evalSamples.limit(1).get('class').getInfo()[0])\n",
        "# these prints will time out occasionally \n",
        "print('Train NP',trainingSamples.filter(ee.Filter.eq('class',0)).size().getInfo())\n",
        "print('Eval NP',evalSamples.filter(ee.Filter.eq('class',0)).size().getInfo())\n",
        "print('Eval P',evalSamples.filter(ee.Filter.eq('class',1)).size().getInfo())\n",
        "print('Train P',trainingSamples.filter(ee.Filter.eq('class',1)).size().getInfo())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTS7_ZzPDhhg",
        "colab_type": "text"
      },
      "source": [
        "Stack the 2D images to create a single image from which samples can be taken.  Convert the image into an array image in which each pixel stores 256x256 patches of pixels for each band.  This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [`neighborhoodToArray()`](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGHYsdAOipa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "featureStack = image\n",
        "\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "arrays = featureStack.neighborhoodToArray(kernel)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4djSxBRG2el",
        "colab_type": "text"
      },
      "source": [
        "Use some pre-made geometries to sample the stack in strategic locations.  Specifically, these are hand-made polygons in which to take the 256x256 samples.  Display the sampling polygons on a map, red for training polygons, blue for evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV890gPHeZqz",
        "colab_type": "text"
      },
      "source": [
        "# Sampling\n",
        "\n",
        "![alt text](https://i.imgur.com/42nx7Zf.png)\n",
        "\n",
        "The mapped data look reasonable so take a sample from each polygon and merge the results into a single export.  The key step is sampling the array image at points, to get all the pixels in a 256x256 neighborhood at each point.  It's worth noting that to build the training and testing data for the FCNN, you export a single TFRecord file that contains patches of pixel values in each record.  You do NOT need to export each training/testing patch to a different image.  Since each record potentially contains a lot of data (especially with big patches or many input bands), some manual sharding of the computation is necessary to avoid the `computed value too large` error.  Specifically, the following code takes multiple (smaller) samples within each geometry, merging the results to get a single export."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ure_WaD0itQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the feature collections to lists for iteration.\n",
        "trainingPolysList = trainingSamples.toList(trainingSamples.size())\n",
        "evalPolysList = evalSamples.toList(evalSamples.size())\n",
        "\n",
        "# These numbers determined experimentally.\n",
        "n = 200 # Number of shards in each polygon.\n",
        "N = 200#0 # Total sample size in each polygon.\n",
        "\n",
        "# Export all the training data (in many pieces), with one task \n",
        "# per geometry.\n",
        "for g in range(trainingSamples.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(trainingPolysList.get(g)).geometry(), \n",
        "      scale = 0.5, \n",
        "      numPixels = N / n, # Size of the shard.\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "  \n",
        "  desc = TRAINING_BASE + '_g' + str(g)\n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc, \n",
        "    bucket = BUCKET, \n",
        "    fileNamePrefix = FOLDER + '/' + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  task.start()\n",
        "\n",
        "# Export all the evaluation data.\n",
        "for g in range(evalSamples.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(evalPolysList.get(g)).geometry(), \n",
        "      scale = 0.5, \n",
        "      numPixels = N / n,\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "  \n",
        "  desc = EVAL_BASE + '_g' + str(g)\n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc, \n",
        "    bucket = BUCKET, \n",
        "    fileNamePrefix = FOLDER + '/' + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  task.start()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}