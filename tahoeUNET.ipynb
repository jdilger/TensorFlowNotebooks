{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNET_tahoe_og.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f6F2MYxpR3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use logging to maintain more detailed information for reproducibility \n",
        "import logging\n",
        "\n",
        "def tfLog():\n",
        "  logging.basicConfig(level=logging.DEBUG, filename='myapp.log',\n",
        "                      format='%(asctime)s %(levelname)s:%(message)s')\n",
        "  try:\n",
        "    logging.debug('######################################')\n",
        "    logging.debug('Config Settings')\n",
        "    logging.debug('######################################')\n",
        "    logging.debug(\"Bucket:%s\",BUCKET) \n",
        "    logging.debug(\"Folder:%s\",FOLDER)\n",
        "    logging.debug('Training base:%s',TRAINING_BASE)\n",
        "    logging.debug('Eaval base:%s',EVAL_BASE) \n",
        "    logging.debug('Band order:%s',BANDS)\n",
        "    logging.debug('Response:%s',RESPONSE)\n",
        "    logging.debug('Features:%s',FEATURES)\n",
        "    logging.debug('Kernal size:%d',KERNEL_SIZE)\n",
        "    logging.debug('FEATURES_DICT:%s',FEATURES_DICT)\n",
        "    logging.debug('Training size:%d',TRAIN_SIZE)\n",
        "    logging.debug('Eval size:%d',EVAL_SIZE)\n",
        "    logging.debug('batch size:%d',BATCH_SIZE)\n",
        "    logging.debug('Epochs:%d',EPOCHS)\n",
        "    logging.debug('Buffer size:%d',BUFFER_SIZE) \n",
        "    logging.debug('Optimizer:%s',OPTIMIZER) \n",
        "    # logging.debug('Loss:',LOSS)\n",
        "    # logging.debug('Other metrics:',METRICS)\n",
        "  except Exception as e:\n",
        "    print('logging failed')\n",
        "    print(e.args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neIa46CpciXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D6ArFWrckmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Earth Engine install to notebook VM.\n",
        "!pip install earthengine-api"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jat01FEoUMqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RnZzcYhcpsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorflow setup.\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "tf.enable_eager_execution()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obDDH1eDzsch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# INSERT YOUR BUCKET HERE:\n",
        "BUCKET = 'ee-tf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7f_9BvcnbOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize inputs\n",
        "def basicNorm(img):\n",
        "  img = img.toFloat();\n",
        "  imgStd = img.reduceRegion(reducer=ee.Reducer.stdDev(),scale=0.5,maxPixels=1e13);\n",
        "  imgMinMax = img.reduceRegion(reducer=ee.Reducer.minMax(),scale=0.5,maxPixels=1e13);\n",
        "  \n",
        "  imgMin = ee.Image.constant(imgMinMax.get(imgMinMax.keys().get(1)));\n",
        "  imgMax = ee.Image.constant(imgMinMax.get(imgMinMax.keys().get(0)));\n",
        "  \n",
        "  normImg = img.subtract(imgMin).divide(imgMax.subtract(imgMin));\n",
        "  return normImg.toFloat()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qs3qZG04u0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras import backend\n",
        "\n",
        "def dice_coeff(y_true, y_pred, smooth=1):\n",
        "    y_true_f = backend.flatten(y_true)\n",
        "    y_pred_f = backend.flatten(y_pred)\n",
        "    intersection = backend.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (backend.sum(y_true_f) + backend.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmfKLl9XcnGJ",
        "colab_type": "text"
      },
      "source": [
        "## Set other global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psz7wJKalaoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras import metrics\n",
        "# Specify names locations for outputs in Cloud Storage. \n",
        "FOLDER = 'tahoe-ogfw-03112020-array-256' #good model'tahoe-ogfw-02292020'\n",
        "TRAINING_BASE = 'Training_tahoe'\n",
        "EVAL_BASE = 'Eval_tahoe'\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "\n",
        "BANDS = ['R','G','B','NIR','L','O','ND']\n",
        "RESPONSE = 'class'\n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "# Sizes of the training and evaluation datasets.\n",
        "TRAIN_SIZE = 16000\n",
        "EVAL_SIZE = 8000\n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 1\n",
        "BUFFER_SIZE = 2000\n",
        "OPTIMIZER = 'Adam'\n",
        "LOSS = dice_loss\n",
        "METRICS = [metrics.get('RootMeanSquaredError'),\n",
        "    metrics.get('MeanAbsoluteError'),\n",
        "    metrics.get('Accuracy'),\n",
        "    dice_coeff,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u57RAvvz2v4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfLog()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8OT9lLv34ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls\n",
        "!cat myapp.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXrvBE4607G",
        "colab_type": "text"
      },
      "source": [
        "# Training data\n",
        "\n",
        "Load the data exported from Earth Engine into a `tf.data.Dataset`.  The following are helper functions for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWZ0UXCVMyJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "  Returns: \n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "  \"\"\"\n",
        "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "  Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "  Returns: \n",
        "    A dtuple of (inputs, outputs).\n",
        "  \"\"\"\n",
        "  inputsList = [inputs.get(key) for key in FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  # Convert from CHW to HWC\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "\n",
        "def get_dataset(pattern):\n",
        "  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "  Get all the files matching the pattern, parse and convert to tuple.\n",
        "  Args:\n",
        "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "  Returns: \n",
        "    A tf.data.Dataset\n",
        "  \"\"\"\n",
        "  glob = tf.gfile.Glob(pattern)\n",
        "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg1fa18336D2",
        "colab_type": "text"
      },
      "source": [
        "Use the helpers to read in the training dataset.  Print the first record to check."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm0qRF0fAYcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_training_dataset():\n",
        "\t\"\"\"Get the preprocessed training dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of training data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + TRAINING_BASE + '*'\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "training = get_training_dataset()\n",
        "\n",
        "# print(iter(training.take(1)).next())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-cQO5RL6vob",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation data\n",
        "\n",
        "Now do the same thing to get an evaluation dataset.  Note that unlike the training dataset, the evaluation dataset has a batch size of 1, is not repeated and is not shuffled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieKTCGiJ6xzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_eval_dataset():\n",
        "\t\"\"\"Get the preprocessed evaluation dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of evaluation data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + EVAL_BASE + '*'\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.batch(1).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "evaluation = get_eval_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JIE7Yl87lgU",
        "colab_type": "text"
      },
      "source": [
        "# Model\n",
        "\n",
        "Here we use the Keras implementation of the U-Net model as found [in the TensorFlow examples](https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb).  The U-Net model takes 256x256 pixel patches as input and outputs per-pixel class probability, label or a continuous output.  We can implement the model essentially unmodified, but will use mean squared error loss on the sigmoidal output since we are treating this as a regression problem, rather than a classification problem.  Since impervious surface fraction is constrained to [0,1], with many values close to zero or one, a saturating activation function is suitable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsnnnz56yS3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import metrics\n",
        "from tensorflow.python.keras import optimizers\n",
        "\n",
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\tencoder = conv_block(input_tensor, num_filters)\n",
        "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\treturn encoder_pool, encoder\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\treturn decoder\n",
        "\n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[None, None, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
        "\toutputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n",
        "\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=optimizers.get(OPTIMIZER), \n",
        "\t\tloss=losses.get(LOSS),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n",
        "\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu_E7OTDBCoS",
        "colab_type": "text"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "You train a Keras model by calling `.fit()` on it.  Here we're going to train for 10 epochs, which is suitable for demonstration purposes.  For production use, you probably want to optimize this parameter, for example through [hyperparamter tuning](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzzaWxOhSxBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = get_model()\n",
        "MODEL_FOLDER = 'test'\n",
        "m.fit(\n",
        "    x=training, \n",
        "    epochs=EPOCHS, \n",
        "    steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE), \n",
        "    validation_data=evaluation,\n",
        "    validation_steps=EVAL_SIZE)\n",
        "\n",
        "modelDir = 'gs://{}/{}/{}'.format(BUCKET,FOLDER,MODEL_FOLDER)\n",
        "\n",
        "tf.contrib.saved_model.save_keras_model(m, modelDir)\n",
        "# TODO: add something to move log to model folder\n",
        "tfLog()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2XrwZHp66j4",
        "colab_type": "text"
      },
      "source": [
        "Note that the notebook VM is sometimes not heavy-duty enough to get through a whole training job, especially if you have a large buffer size or a large number of epochs.  You can still use this notebook for training, but may need to set up an alternative VM ([learn more](https://research.google.com/colaboratory/local-runtimes.html)) for production use.  Alternatively, you can package your code for running large training jobs on Google's AI Platform [as described here](https://cloud.google.com/ml-engine/docs/tensorflow/trainer-considerations).  The following code loads a pre-trained model, which you can use for predictions right away."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YcsnqOA2du_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.tools import saved_model_utils\n",
        "modelDir = 'gs://{}/{}/model-dice-256'.format('ee-tf',FOLDER)\n",
        "\n",
        "meta_graph_def = saved_model_utils.get_meta_graph_def(modelDir, 'serve')\n",
        "inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
        "outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
        "\n",
        "# Just get the first thing(s) from the serving signature def.  i.e. this\n",
        "# model only has a single input and a single output.\n",
        "input_name = None\n",
        "for k,v in inputs.items():\n",
        "  input_name = v.name\n",
        "  break\n",
        "\n",
        "output_name = None\n",
        "for k,v in outputs.items():\n",
        "  output_name = v.name\n",
        "  break\n",
        "\n",
        "# Make a dictionary that maps Earth Engine outputs and inputs to \n",
        "# AI Platform inputs and outputs, respectively.\n",
        "import json\n",
        "input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
        "output_dict = \"'\" + json.dumps({output_name: \"class\"}) + \"'\"\n",
        "\n",
        "# Put the EEified model next to the trained model directory.\n",
        "# TODO: add eeidied dir, project into to log, add output name\n",
        "EEIFIED_DIR = '{}/eeified'.format(modelDir)\n",
        "PROJECT = 'tf-workshop-253517'\n",
        "print(input_dict,output_dict)\n",
        "# You need to set the project before using the model prepare command.\n",
        "!earthengine set_project {PROJECT}\n",
        "!earthengine model prepare --source_dir {modelDir} --dest_dir {EEIFIED_DIR} --input {input_dict} --output {output_dict}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E0OpHQo5DKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time \n",
        "MODEL_NAME = 'eeified_03112020_dice_256'\n",
        "# PROJECT = 'tf-workshop-253517'\n",
        "VERSION_NAME = 'v' + str(int(time.time()))\n",
        "print('Creating version: ' + VERSION_NAME)\n",
        "\n",
        "!gcloud ai-platform models create {MODEL_NAME} --project {PROJECT}\n",
        "!gcloud ai-platform versions create {VERSION_NAME} \\\n",
        "  --project {PROJECT} \\\n",
        "  --model {MODEL_NAME} \\\n",
        "  --origin {EEIFIED_DIR} \\\n",
        "  --runtime-version=1.14 \\\n",
        "  --framework \"TENSORFLOW\" \\\n",
        "  --python-version=3.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RJpNfEUS1qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a trained model. \n",
        "MODEL_DIR = 'gs://ee-tf/tahoe-ogfw-02292020/model-ogwf-256'\n",
        "m = tf.contrib.saved_model.load_keras_model(MODEL_DIR)\n",
        "help(m.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}